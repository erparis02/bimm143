---
title: "Class 10"
author: "Emily R. Paris"
date: "2/6/2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# PCA Objectives

Visualize multi-dimensional variables, chose most useful variables, ID groups of objects, ID outliers.

## PCA Issues

Data in different scales. To argue with scaling, use **'prcomp(x, scale = TRUE)'**. 'Prcomp()' = principal component analysis (PCA). With scale = FALSE, some numbers will overcome other data points.


### Unsupervised Learning Analysis of Human Breast Cancer Cells

```{r}
# Import the Data
wisc.df <- read.csv("WisconsinCancer.csv")

# Examine data
head(wisc.df)
```

There are some funky things in this dataset that we will ignore for our analysis. This includes ID and Diagnosis columns and the funky last 'X' column.

```{r}
# Convert the features of the data: wisc.data; rows on left of [,], columns on right
wisc.data <- as.matrix(wisc.df[,3:32])
```

> Q: How many patients do we have data for?

```{r}
nrow(wisc.data)
```


> Q: How many cancer and non-cancer patients?

```{r}
table(wisc.df$diagnosis)
```

> Q: How many variables/features in the data are suffixed with _mean?

```{r}
# Grep() needs vector, all column names in data: colnames(wisc.data)
# Pattern finding
grep("_mean", colnames(wisc.data), value = TRUE)
```

```{r}
# Sum of total number of "averages" columns
length(grep("_mean", colnames(wisc.data), value = TRUE))
```

#### Performing PCA on this data:

Before performing PCA on this data, we need to consider whether we should SCALE our input.

It is important to check if the data need to be scaled before performing PCA. Recall two common reasons for scaling data include:

The input variables use different units of measurement.
The input variables have significantly different variances.
Check the mean and standard deviation of the features (i.e. columns) of the wisc.data to determine if the data should be scaled. Use the 'colMeans()' and 'apply()' functions like you’ve done before.


```{r}
# Check Means
round(colMeans(wisc.data), 2)
```


```{r}
# Check Standard Deviations
# 'Round()' = round to 2 decimals
round(apply(wisc.data, 2 ,sd), 2)
```

Looks like we need to set scale=TRUE!!

```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp( wisc.data, scale=TRUE )
summary(wisc.pr)

# Example of Data Interpretation: "PC1 captures 44% of all variance in data (looking at 'Proportion of Variance' row).

#Cumulative Proportion means adding the Proportion of Variance (POV) to the previous POV of the PCA before.
```


> Q: From your results, what proportion of the original variance is captured by the first principal components (PC1)?

The 1st PC captures 44.27% of the original variance.

> Q: How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3: 70% is captured in the first 3 PC's

> Q: How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7: 90% is captured in the first 7 PC's


###### Figures for Cancer Data

Sometimes biplot is useful, but there are too many data points here.
```{r}
biplot(wisc.pr)
```


We need to make our own PC1 vs PC2 plot, colored by expert diagnosis.

```{r}
# Shows what things are in results
attributes(wisc.pr)
```

```{r}
# Based on spread, we can see that there are lots of ways to be named malignant, but not as many to be benign.
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = wisc.df$diagnosis)

# Ablines at 0; point further from 0 are more different
abline(h=0, col="gray", lty=2)
abline(v=0, col="gray", lty=2)

```


### Cluster in PC Space

First let's see if we can cluster the original data.

```{r}
wisc.hc <- hclust(dist(wisc.data))
plot(wisc.hc)
```
This is not very helpful--branches are weird!


Let’s see if PCA improves or degrades the performance of hierarchical clustering.

Using the minimum number of principal components required to describe at least 90% of the variability in the data, create a hierarchical clustering model with the linkage method="ward.D2". We use Ward’s criterion here because it is based on multidimensional variance like principal components analysis. Assign the results to wisc.pr.hclust.

```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:3]), method = "ward.D2")
plot(wisc.pr.hclust)
```


To get clusters out of this tree we need to CUT it with the 'cutree()' function.

```{r}
grps3 <- cutree(wisc.pr.hclust, k=2)
table(grps3)
```

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=grps3)
```

We can use 'table()' function to compare $diagnosis vector with our cluster results vector.

Some malignant and benign tumors were misdiagnosed. These are represented by the red points "invading" the black points cluster on the first black and red 2D plot above.

```{r}
table(grps3, wisc.df$diagnosis)
```

#####Prediction
We will use the predict() function that will take our PCA model from before and new cancer cell data and project that data onto our PCA space.

```{r}
new <- read.csv("new_samples.csv")
new
```

Use the 'predict()' function eith our previous PCA model and new data...
```{r}
npc <- predict(wisc.pr, newdata=new)
npc
```

Now draw the PCA plot again and add our new data:

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = wisc.df$diagnosis)
points(npc[,1], npc[,2], col="blue", pch=15, cex = 3)
text(npc[,1], npc[,2], labels = c(1,2), col="white")
```

Now we know to prioritize patient 2 because they are in the malignant zone!




















